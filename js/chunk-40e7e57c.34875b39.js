(window["webpackJsonp"]=window["webpackJsonp"]||[]).push([["chunk-40e7e57c"],{"09c8":function(e,t,n){"use strict";var i=n("a8b1"),a=n.n(i);a.a},"5d90":function(e,t,n){e.exports=n.p+"img/Andro.fcb3acf3.svg"},a5a3:function(e){e.exports=JSON.parse('{"stamps":[{"key":1,"gtag":"D0","title":"Keras & Model Creation","url":"https://github.com/draymond63/Andro/blob/master/models/AndroD0.py","blurb":"The model is built using Keras, Tensorflow, & Larq (quantizing library)","code":"# 81 kB required minimum\\nmodel = tf.keras.models.Sequential([\\n    tf.keras.layers.Flatten(),\\n    lq.layers.QuantDense(512,\\n        input_shape=(784,),\\n        input_quantizer=\'ste_sign\',\\n        kernel_quantizer=\'ste_sign\',\\n        kernel_constraint=\'weight_clip\'\\n    ),\\n    lq.layers.QuantDense(10,\\n        input_quantizer=\'ste_sign\',\\n        kernel_quantizer=\'ste_sign\',\\n        kernel_constraint=\'weight_clip\',\\n        activation=\'softmax\'\\n    ),\\n])"},{"key":2,"gtag":"D1","title":"Proof of Reloading","url":"https://github.com/draymond63/Andro/blob/master/models/AndroD1.py","blurb":"This model reads in saved model and binarizes weights to test accuracy","code":"for i, layer in enumerate(model.layers):\\n    temp_weights = np.asarray(weights[i], dtype=np.float32)\\n    temp_biases = np.asarray(biases[i], dtype=np.float32)\\n    # Polarize every weight\\n    for x, node in enumerate(temp_weights):\\n        for y, weight in enumerate(node):\\n            if weight > 0:\\n                temp_weights[x][y] = 1\\n            else:\\n                temp_weights[x][y] = -1"},{"key":3,"gtag":"D2","title":"Numpy Matrix Multiplication","url":"https://github.com/draymond63/Andro/blob/master/models/AndroD2.py","blurb":"Treats each layer as a matrix that it multiplied by the previous until the result it a 1x10 vector of answers, completely removing all AI frameworks from the process","code":"def model(image, use_biases=True):\\n    layer = quantize(image)\\n    for layer_weights, layer_biases in zip(weights, biases):\\n        layer_weights = np.array(layer_weights, dtype=np.float32)\\n        # Stil 78.72% accuracy without biases? (Same as with)\\n        if use_biases: \\n            layer_biases = np.array(layer_biases, dtype=np.float32)\\n        else:\\n            layer_biases = 0\\n        layer = np.matmul(layer, layer_weights) + layer_biases\\n        # Quantize all but the last layer\\n        if len(layer[0]) != 10:\\n            layer = quantize(layer)"},{"key":4,"gtag":"D3","title":"Layer Driven Multiplication","url":"https://github.com/draymond63/Andro/blob/master/models/AndroD3.py","blurb":"Re-organize loop methods to iterate through each layer, node, then weight - no libraries involved","code":"# Iterate through layers, moving linearized image to 10 node output\\nfor layer_weights, layer_biases in zip(weights, biases):\\n    output_layer = []\\n    # Iterate through each of the nodes in the current layer\\n    for node_weights, node_bias in zip(layer_weights, layer_biases):\\n        node_val = 0\\n        # Iterate through previous layer, multiplying previous nodes \\n        # by the weights and putting it into the new node\\n        for data, weight in zip(input_layer, node_weights):\\n            # Take the MSB of the val if required\\n            if quanitizing:\\n                node_val += quantize(weight) * quantize(data)\\n            else:\\n                node_val += weight * data"},{"key":5,"gtag":"D4","title":"Bitpacked Proof of Concept","url":"https://github.com/draymond63/Andro/blob/master/models/AndroD4.py","blurb":"Using bitwise operators, bit unpacking and repacking to show how software would do the hardware\'s job","code":"# Calculation of one node\\nfor w_index in range(prev_size):\\n    EEPROM_addr = w_index >> 3  # 3 LSBs are for bit selection\\n    bit_index = w_index & 0b111 # Mask away all but lowest three bits\\n    # Select bit weight and input data\\n    # print(bin(weights[l_index][node][EEPROM_addr]))\\n    weight = weights[l_index][node][EEPROM_addr] & ( 1 << bit_index )\\n    data = input_layer[EEPROM_addr]              & ( 1 << bit_index )\\n    # Multiply and add\\n    mult = XNOR(weight, data)\\n    accum = UpDown(accum, mult) # * Breakpoint here to match D5"},{"key":6,"gtag":"D5","title":"Fully Fledged hardware sim","url":"https://github.com/draymond63/Andro/blob/master/models/AndroD5/LogitCalculator.py","blurb":"Using a personally-built hardware simulation library to replicate the hardware circuit before purchasing parts","code":"def _initSrRestore(self):\\n    # SRs\\n    self.I1_SR = IC.ShiftRegister(name=\'I1-SR\')\\n    self.I2_SR = IC.ShiftRegister(name=\'I2-SR\')\\n    # AND write and clk to know when to shift in\\n    I1_SR_CLK = asyncIC.AND(1)\\n    I2_SR_CLK = asyncIC.AND(1)\\n    # Active when EEPROM is being written to (OTHER EEPROM IS BEING READ FROM)\\n    I1_SR_CLK.wire(self.node_done.output, self.I2_RD)\\n    I2_SR_CLK.wire(self.node_done.output, self.I1_RD)\\n    # Wire input of SRs to LSB of accum\\n    i = self.accum.width\\n    MSB = self.accum.output[i-1:i] # Take the MSB\\n    quantI = asyncIC.NOT(name=\'Accum-Quantized\')\\n    quantI.wire(MSB)"}]}')},a63c:function(e,t,n){},a8b1:function(e,t,n){},bb42:function(e,t,n){"use strict";n.r(t);var i=function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"an-main",class:{"main-mobile":this.is_mobile}},[e._m(0),i("img",{class:{"img-mobile":this.is_mobile},attrs:{src:n("5d90")}}),i("timeline",{staticClass:"tl-mobile",class:{"tl-desktop":!this.is_mobile},attrs:{items:n("a5a3"),is_mobile:this.is_mobile},on:{"ga-event":function(t,n,i){return e.$emit(t,n,i)}}})],1)},a=[function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",[n("span",{attrs:{id:"header"}},[n("h1",[e._v("Andro")]),n("h2",[e._v("A binarized neural network breadboard processor")])]),n("h3",[e._v("The Why")]),n("p",[e._v(" The goal here is to breakdown Artifical Intelligence to the bare necessities - a sequence of neurons arranged into layers, all with weights and biases tied to them. To simplify the project, all the weights must be either 1 or -1. ")]),n("h3",[e._v("The What")]),n("p",[e._v(" A circuit has been devised to implement the matrix multiplication of dense layer neural networks. In the end, the model will be stored on a chip that can be used to multiply the incoming data according, resulting in the final prediction. ")]),n("h3",[e._v("The Timeline")]),n("p",[e._v(" Below is how I've taken a Keras model and deconstructed it to simulate the hardware I am currently implementing. ")])])}],s=function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("span",{staticClass:"tl-main"},[n("div",{staticClass:"line"}),e._l(e.stamps,(function(t){return n("div",{key:t.key,staticClass:"stamp-format"},[n("span",{staticClass:"stamp"},[n("h3",[n("a",{attrs:{href:e.formatURL(t.url)},on:{click:function(n){return e.$emit("ga-event","Andro",t.gtag)}}},[e._v(e._s(t.title))])]),n("p",[e._v(e._s(t.blurb))]),n("pre",{directives:[{name:"highlightjs",rawName:"v-highlightjs",value:t.code,expression:"stamp.code"}],staticClass:"code"},[e._v("        "),n("code",{staticClass:"python"}),e._v("\n      ")])]),n("div",{staticClass:"bullet"},[n("h2",{staticStyle:{"margin-top":"0.2em","font-size":"1.2em"}},[e._v(e._s(t.key))])])])}))],2)},r=[],o={name:"Timeline",props:{items:Object},data:function(){return{stamps:[]}},mounted:function(){this.stamps=this.items["stamps"]},methods:{formatURL:function(e){return""!=e?e:null}}},l=o,d=(n("09c8"),n("2877")),u=Object(d["a"])(l,s,r,!1,null,"bf2a7dd8",null),h=u.exports,c={name:"AndroPage",components:{Timeline:h},props:{is_mobile:Boolean}},m=c,p=(n("d9f6"),Object(d["a"])(m,i,a,!1,null,"bb724c5a",null));t["default"]=p.exports},d9f6:function(e,t,n){"use strict";var i=n("a63c"),a=n.n(i);a.a}}]);
//# sourceMappingURL=chunk-40e7e57c.34875b39.js.map